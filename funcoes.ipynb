{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python383jvsc74a57bd09a855d225e8ebf7c4889a92d6065f44a3a657bc62a83a021168843adb770e43e",
   "display_name": "Python 3.8.3 64-bit (conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TCC da Pós-graduação Lato Sensu em Ciência de Dados e Big Data\n",
    "## Entidade promotora: PUC Minas Virtual\n",
    "### Trabalho de Conclusão de Curso apresentado ao Curso de Especialização em Ciência de Dados e Big Data como requisito parcial à obtenção do título de especialista.\n",
    "### Projeto: Preditores de óbitos por COVID-19: O aprendizado de máquina como instrumento auxiliar na definição de políticas públicas\n",
    "\n",
    "### Aluno: Breno Marques Barreto\n",
    "\n",
    "# Considerações importantes sobre a utilização dos modelos salvos:\n",
    "# Deve-se conferir se a versão do Python que irá utilizar é a mesma usada neste projeto.\n",
    "# Aconselha-se utilizar para deserializar o modelo salvo:\n",
    "#   1 - A mesma versão do Python. \n",
    "#   2 - As mesmas versãoes das bibliotecas usadas no projeto. \n",
    "# As orientações não são apenas para as versões da NumPy e da scikit-learn."
   ]
  },
  {
   "source": [
    "# BIBLIOTECAS"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bibliotecas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import os.path\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score ,roc_curve,auc\n",
    "from sklearn.model_selection import GridSearchCV,StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "seed=45\n",
    "%matplotlib inline\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "source": [
    "# CARGA DOS DADOS"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################\n",
    "# Esta Função carrega os dados de um arquivo csv para um dataframe \n",
    "################################################################# \n",
    "def load_df(filename,psep,pusecols=None,pconverters=None):\n",
    "    \"\"\"Esta carrega os dados de um arquivo csv para um dataframe.\n",
    "          pusecols - limita as colunas a serem carregadas.\n",
    "          pconverters - define a forma de conversão de dados\"\"\"\n",
    "    usecols = pusecols\n",
    "    converters = pconverters\n",
    "    df = pd.read_csv(filename, sep=psep, usecols=pusecols, converters=pconverters)\n",
    "    return df"
   ]
  },
  {
   "source": [
    "# PROCESSAMENTO"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "variavel_alvo = ['OBT_COVID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################\n",
    "# Função para converter os tipos de dados não identificados pelo pandas e otimizar o processamento \n",
    "#################################################################\n",
    "def conv(val):\n",
    "    \"\"\"Função para converter os tipos de dados não identificados pelo pandas e otimizar o processamento.\"\"\"\n",
    "    if not val:\n",
    "        return 0    \n",
    "    try:\n",
    "        return np.float64(val)\n",
    "    except:        \n",
    "        return np.float64(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################\n",
    "# Esta função realiza a codificação dos dados \n",
    "#################################################################\n",
    "def coding(col, codeDict):\n",
    "  \"\"\"Esta função realiza a codificação dos dados.\"\"\"\n",
    "  colCoded = pd.Series(col, copy=True)\n",
    "  for key, value in codeDict.items():\n",
    "    colCoded.replace(key, value, inplace=True)\n",
    "  return colCoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################\n",
    "# Esta função seleciona as variáveis \n",
    "# conforme o nível mínimo de variância estipulado por threshold\n",
    "#################################################################\n",
    "def elimina_baixa_variancia(X_data,threshold=0.1):\n",
    "    \"\"\"Esta função Seleciona, no dataset de treino,\n",
    "       as variáveis conforme o nível mínimo de variância estipulado por threshold.\"\"\"\n",
    "    print(\"Num. variáveis inicial:{}\".format(len(X_data.columns)))\n",
    "    selector = VarianceThreshold(threshold)\n",
    "    selector.fit(X_data)\n",
    "    print(\"Num. variáveis final:{}\".format(len(X_data.columns)))    \n",
    "    return X_data[X_data.columns[selector.get_support(indices=True)]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################\n",
    "# Esta função corrige idades com valores negativos\n",
    "#################################################################\n",
    "def corrige_idade(covid_df):\n",
    "    \"\"\"Esta função corrige idades com valores negativos.\"\"\"\n",
    "    # Há duas abordagens: substituir pela média ou pelo valor absoluto. \n",
    "    # Esta hipótese pode gerar distorções se houver valores muito grandes.\n",
    "    # Optou-se pela média\n",
    "    covid_df.NU_IDADE_N [covid_df.NU_IDADE_N<0] = covid_df.NU_IDADE_N.mean()\n",
    "    return covid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################\n",
    "# Esta função cria uma variável categórica baseada na idade (NU_IDADE_N)\n",
    "#################################################################\n",
    "def cria_faixa_etaria(covid_df):\n",
    "    \"\"\"Esta função cria variável categórica baseada na idade (NU_IDADE_N)\"\"\"\n",
    "    pbins=[0, 20, 29, 39, 49, 59, 69, 79, sys.maxsize]\n",
    "    plabels=['0-20', '20-29', '29-39', '39-49','49-59', '59-69','69-79','79-150']\n",
    "    covid_df['NU_IDADE_CAT'] = pd.cut(covid_df['NU_IDADE_N'], bins=pbins, labels=plabels, include_lowest=True) \n",
    "    codigos_idade = {'0-20':0, '20-29':1, '29-39':2, '39-49':3,'49-59':4, '59-69':5,'69-79':6,'79-150':7}\n",
    "    covid_df['NU_IDADE_CAT'] = coding(covid_df['NU_IDADE_CAT'], codigos_idade).astype(int)  \n",
    "    return covid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################\n",
    "# Esta função une os dois datasets através das colunas CO_MUN_NOT\n",
    "#################################################################\n",
    "def merge_dfs(geoses_df, covid_df):\n",
    "    \"\"\"Esta função une os dois datasets através das colunas CO_MUN_NOT\"\"\"\n",
    "    # Renomeia a coluna de geoses_df que servirá para o merge\n",
    "    geoses_df.rename(columns={'v0002_codigo_do_municipio': 'CO_MUN_NOT'}, inplace = True)\n",
    "    # Seleciona apenas as colunas relevantes\n",
    "    columns = ['CO_MUN_NOT','GeoSES']\n",
    "    geoses_df = geoses_df[columns]\n",
    "\n",
    "    # As tabelas geoses_df e covid_df serão unidas (merge) através do campo \"CO_MUN_NOT\",\n",
    "    # que possui formatos diferentes nelas. Em geoses_df possui 7 dígitos,\n",
    "    # como nas tabelas do IBGE, e 6 dígitos em covid_df.\n",
    "    # O último dígito não é, aparentemente, significante. \n",
    "    # Decidiu-se retirá-lo daquela tabela e adequar o campo ao tamanho da tabela covid_df, 6 dígitos\n",
    "    geoses_df['CO_MUN_NOT'] = geoses_df['CO_MUN_NOT'].apply(lambda x: str(x)[:-1]).astype(int)\n",
    "\n",
    "    # Confirmar que não há registros duplicados em geoses_df, garantindo-se a integridade.\n",
    "    print(\"Total de registros em geoses_df:\",len(geoses_df.groupby ('CO_MUN_NOT').CO_MUN_NOT.count()))\n",
    "\n",
    "    # Une os dois dataframes através do campo CO_MUN_NOT\n",
    "    merged_df = covid_df.merge(geoses_df, on='CO_MUN_NOT')\n",
    "\n",
    "    # Exclusão da variavel \"CO_MUN_NOT\". \n",
    "    merged_df = merged_df.drop(['CO_MUN_NOT'], axis = 1)\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################\n",
    "# Esta função define a variável alvo como decorrência de 'CLASSI_FIN' e 'EVOLUCAO'\n",
    "#######################################################################################\n",
    "def define_variavel_alvo(covid_df):\n",
    "    \"\"\"Esta função define a variável alvo como decorrência de 'CLASSI_FIN' e 'EVOLUCAO' \"\"\"\n",
    "    # CLASSI_FIN COVID e EVOLUCAO para óbito será a variavel a ser predita\n",
    "    condicao=[(covid_df['CLASSI_FIN']==5.0) & (covid_df['EVOLUCAO']==2.0), (covid_df['EVOLUCAO']!=2.0)]\n",
    "    resultados = [1, 0]\n",
    "    # Se o registro foi classificado como COVID e evoliu para óbito, OBT_COVID será preenchida como 1, caso contrário 0.\n",
    "    covid_df['OBT_COVID']=np.select(condicao, resultados)\n",
    "    # Exclusão da variavel \"EVOLUCAO\". Não pode ser mantida pois resultaria em acurácia de 100%.\n",
    "    # Foi substituída por OBT_COVID.\n",
    "    covid_df=covid_df.drop(['EVOLUCAO'], axis = 1)\n",
    "    return covid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################\n",
    "# Esta Função converte os tipos das colunas com até 10 valores distintos em categoricas\n",
    "#######################################################################################\n",
    "def converte_tipo_category(covid_df,list_cols=None):\n",
    "    \"\"\"Esta função converte o tipo das colunas com até 10 valores distintos em categorica\"\"\"\n",
    "    if (list_cols is None):\n",
    "        col = covid_df.columns\n",
    "    else:\n",
    "        col = list_cols\n",
    "    covid_df_temp = covid_df.copy()    \n",
    "    for i in col:\n",
    "        if covid_df[i].nunique()<=10:     #Filtra pelo número de ocorrências as que serão convertidas\n",
    "            covid_df_temp[i] = covid_df[i].astype('category')\n",
    "    return covid_df_temp\n"
   ]
  },
  {
   "source": [
    "### NORMALIZAÇÃO DOS DADOS"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler # importa a biblioteca para execução da normalização/reescala\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "########################################################\n",
    "# Esta função realiza a reescala dos atributos contínuos\n",
    "########################################################\n",
    "def normaliza_dados(pcovid_df,list_cols=None):\n",
    "    \"\"\"Esta função realiza a reescala dos atributos contínuos\"\"\"\n",
    "\n",
    "    covid_df = pcovid_df.copy()\n",
    "    if (list_cols is None):\n",
    "        num_col = covid_df.select_dtypes(include =['float64','int64','int32']).columns\n",
    "    else:\n",
    "        num_col = list_cols\n",
    "    #gera uma cópia do dataframe\n",
    "    covid_df_adjusted_MinMaxScaler = covid_df.copy()  \n",
    "    #executa a reescala/normalização\n",
    "    covid_df_adjusted_MinMaxScaler[num_col] = MinMaxScaler().fit_transform(covid_df_adjusted_MinMaxScaler[num_col]) \n",
    "    return covid_df_adjusted_MinMaxScaler"
   ]
  },
  {
   "source": [
    "### ANÁLISE DA CORRELAÇÃO"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################\n",
    "# Esta função calcula a correlação entre as variáveis do dataframe com a variável \n",
    "# alvo e retornar as com correlação maior que o parâmetro escolhido \n",
    "#######################################################################################\n",
    "def verifica_correlacao(covid_df,grafico=False,parametro=0.01):\n",
    "    \"\"\"Esta função calcula a correlação entre as variáveis do daataframe\n",
    "       e rotorna as que estão acia de determinado valor\"\"\"\n",
    "    # Correlacao com a variavel alvo usando a Correlação de Pearson \n",
    "    plt.figure(figsize=(30,25))\n",
    "    cor = covid_df.corr()\n",
    "    sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)\n",
    "    if (grafico == True):\n",
    "        plt.show()\n",
    "    cor_target = abs(cor['OBT_COVID'])\n",
    "    #Selecionadas as variáveis com maior correlação\n",
    "    relevant_features = cor_target[cor_target>parametro].sort_values(ascending=False)\n",
    "    return relevant_features"
   ]
  },
  {
   "source": [
    "### CONVERSÃO DAS VARIÁVEIS CATEGÓRICAS EM BINÁRIAS"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "####################################################################################\n",
    "# Esta função converte os atributos categóricos para binários, exceto a variável alvo.\n",
    "####################################################################################\n",
    "def converte_binarios(covid_df,list_cols=None):\n",
    "    \"\"\"Esta função converte os atributos categóricos para binários, exceto a variável alvo\"\"\"\n",
    "    if (list_cols is None):\n",
    "        # Seleciona todas as variáveis categóricas do dataframe exceto a alvo.\n",
    "        cat_col = covid_df.select_dtypes(include =['category']).columns.difference(['OBT_COVID']).values \n",
    "    else:\n",
    "        # Seleciona todas as colunas recebidas\n",
    "        cat_col = list_cols\n",
    "    # Converte as variáveis categóricas em binárias\n",
    "    covid_df_bin = pd.get_dummies(covid_df, columns=cat_col)  # cat_col contém os atributos categóricos.\n",
    "    # Reordena a coluna \"OBT_COVID\" para ser a ultima, facilitando a separação da variável alvo\n",
    "    cols = [col for col in covid_df_bin if col != 'OBT_COVID'] + ['OBT_COVID']\n",
    "    covid_df_bin = covid_df_bin[cols]\n",
    "    return covid_df_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################\n",
    "#Esta função exibe as ocorrências de valores únicos em cada coluna da dataframe\n",
    "####################################################################################\n",
    "def unicos(covid_df):\n",
    "    \"\"\"Esta função exibe as ocorrências de valores únicos em cada coluna da dataframe\"\"\"\n",
    "    col = covid_df.columns\n",
    "    k = pd.DataFrame(index=col)\n",
    "    for i in col:\n",
    "        k['No de únicos'] = covid_df[i].nunique()\n",
    "        k['Prim. valor unico'] = covid_df[i].unique()[0]\n",
    "        k['Seg. valor unico'] = covid_df[i].unique()[1]\n",
    "        return k.T"
   ]
  },
  {
   "source": [
    "### BALANCEAMENTO DA BASE DE DADOS"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from imblearn.over_sampling import SMOTENC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "##############################################################################\n",
    "# Esta função realiza o balanceamento das classes alvo através do método SMOTE\n",
    "##############################################################################\n",
    "def imblearn_SMOTE_oversampling(covid_df, filename=None, reprocessa=False, ptarget='OBT_COVID'):\n",
    "    \"\"\"Esta função realiza o balanceamento das classes alvo através do método SMOTE\n",
    "       Parametros: covid_df: dataframe desbalanceado e string com nome da variável alvo\n",
    "                   filename: nome do arquivo antes do processo de oversampling\n",
    "                   reprocessa: se o dataframe deve ser reprocessado mesmo que o processado exista\n",
    "                   ptarget : nome da variável alvo\n",
    "      Retorno: Dataframe balanceado através do SMOTE da biblioteca imblearn\n",
    "      Se já existe o arquivo processado, apenas carrega\n",
    "    \"\"\"\n",
    "    numfeatures = str(covid_df.shape[1]) # será usado na formacao do nome do arquivo csv\n",
    "    if (filename is None):  # se não foi passado nome de arquivo, efetua o balanceamento do dataframe\n",
    "        print(\"Nome de arquivo não informado.\")\n",
    "        print(\"Aguarde, efetuando processamentofor do dataframe...\")\n",
    "        print(covid_df[ptarget].value_counts())\n",
    "        X = covid_df.drop(ptarget, axis=1)\n",
    "        Y = covid_df[ptarget]\n",
    "        sm = SMOTE(random_state=42)\n",
    "        X_res, Y_res = sm.fit_resample(X, Y)\n",
    "        df_smote_over = pd.concat([pd.DataFrame(X_res), pd.DataFrame(Y_res, columns=[ptarget])], axis=1)\n",
    "        print('SMOTE over-sampling:')\n",
    "        print(df_smote_over[ptarget].value_counts())\n",
    "        #salva o arquivo balanceado\n",
    "        filename_pre =\"covid_df\" + numfeatures + \"_balanceado.csv\"\n",
    "        df_smote_over.to_csv(filename_pre,index = False, sep=';')\n",
    "    else:  # foi passado o nome de um arquivo da base desbalanceada\n",
    "            os.path.isfile(filename)\n",
    "            obj_filename = Path(filename)\n",
    "            print(\"Arquivo informado:\" ,filename)\n",
    "            #monta o nome do arquivo da base balanceada\n",
    "            ext_file = \"_proc_smote_over.csv\"\n",
    "            rad_filename = filename[:-4]  #exclui a extensão do arquivo\n",
    "            filename_pre = rad_filename + \"_\" + numfeatures + ext_file\n",
    "            if obj_filename.is_file():  # se o arquivo da base não balanceada existe\n",
    "                #verifica se o arquivo processado existe\n",
    "                os.path.isfile(filename_pre)\n",
    "                file_filename_pre = Path(filename_pre)\n",
    "                print(\"Verifica existencia do arquivo:\" ,file_filename_pre.is_file())\n",
    "                if file_filename_pre.is_file() and reprocessa==False: # se existe e não reprocessar\n",
    "                    #carrega o arquivo csv já balanceado\n",
    "                    print(\"Arquivo {} foi encontrado.\".format(file_filename_pre))\n",
    "                    print(\"Carregando o arquivo...\")\n",
    "                    df_smote_over = load_df(file_filename_pre,\";\",None,None)\n",
    "                    print(\"Concluído.\")\n",
    "                else:  # o arquivo preprocessado não existe ou o balanceamento deve ser refeito\n",
    "                    #Faz o balanceamento dos dados\n",
    "                    print(\"Arquivo {} não encontrado.\".format(file_filename_pre))\n",
    "                    print(\"Aguarde, efetuando balanceamento das classes da variável alvo...\")\n",
    "                    print(covid_df[ptarget].value_counts())\n",
    "                    X = covid_df.drop(ptarget, axis=1)\n",
    "                    Y = covid_df[ptarget]\n",
    "                    sm = SMOTE(random_state=42)\n",
    "                    X_res, Y_res = sm.fit_resample(X, Y)\n",
    "                    df_smote_over = pd.concat([pd.DataFrame(X_res),pd.DataFrame(Y_res,columns=[ptarget])],axis=1)\n",
    "                    print('SMOTE over-sampling:')\n",
    "                    print(df_smote_over[ptarget].value_counts())\n",
    "                    #salva o arquivo balanceado\n",
    "                    df_smote_over.to_csv(filename_pre,index = False, sep=';')\n",
    "            else:  #O arquivo da base passada como nao balanceada nao existe\n",
    "                print(\"O arquivo {} referenciado como base desabalanceada não existe.\".format(filename))\n",
    "                print(\"Forneça um nove de arquivo válido.\")\n",
    "    #Exibe gráfico com as classes balanceadas\n",
    "    df_smote_over[ptarget].value_counts().plot(kind='bar', title='Classes OBT_COVID') \n",
    "    return df_smote_over \n"
   ]
  },
  {
   "source": [
    "# CRIAÇÃO DOS MODELOS"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import imblearn\n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.under_sampling import ClusterCentroids\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.under_sampling import EditedNearestNeighbours\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import plot_confusion_matrix, classification_report, accuracy_score\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################\n",
    "# Esta função cria uma matriz de correlação com as variáveis do dataframe\n",
    "####################################################################################\n",
    "def matrix_correlacao(covid_df,pbest_feat=None, ptarget='OBT_COVID'):\n",
    "    \"\"\"Esta função cria uma matriz de correlação com as variáveis do dataframe.   \n",
    "    \"\"\"\n",
    "    if (pbest_feat is None):\n",
    "        best_feat = covid_df.columns\n",
    "    else:\n",
    "        best_feat = pbest_feat + [ptarget] \n",
    "    print(\"Melhores features\",best_feat)\n",
    "    X = covid_df.iloc[:,:-1]  #variáveis independentes\n",
    "    y = covid_df.iloc[:,-1].astype(int) #variável alvo, última coluna. Converte para int devido a SelectKBest\n",
    "    #Obtém as correlações de cada variavel no dataset\n",
    "    corrmat = covid_df[best_feat].corr()\n",
    "    #print(corrmat)\n",
    "    top_corr_features = corrmat.index\n",
    "    plt.figure(figsize=(35,35))\n",
    "    #plot heat map\n",
    "    g=sns.heatmap(covid_df[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# Estas funções serializam e deserializam os modelos criados \n",
    "############################################################\n",
    "from numpy import loadtxt\n",
    "import pickle\n",
    "\n",
    "def save_model(pmodel, pmodel_name):\n",
    "    \"\"\"Esta função salva o modelo.\"\"\"\n",
    "    pickle.dump(pmodel, open(pmodel_name, \"wb\"))\n",
    "    return\n",
    "\n",
    "def load_model(pmodel_name):\n",
    "    \"\"\"Esta função carrega um modelo.\"\"\"\n",
    "    pickle.load(open(pmodel_name, \"rb\"))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################\n",
    "#      Esta função recebe o dataset de treino \n",
    "#       e um tipo de classificador. O treina\n",
    "#       com os hiperparâmetros já ajustados \n",
    "#       e retorna o modelo treinado \n",
    "###########################################################\n",
    "def cria_modelo(X_train,y_train,classificador,tuned=False):\n",
    "    \"\"\"Esta função recebe o dataset de treino \n",
    "       e um tipo de classificador. Dependendo do\n",
    "       valor do parametro tuned, cria e treina\n",
    "       com os hiperparâmetros já ajustados \n",
    "       e o retorna.   \n",
    "    \"\"\"\n",
    "\n",
    "    if classificador==\"1\":\n",
    "        # XGBClassifier\n",
    "        # Ajustados com HalvingRandomSearchCV no notebook “COVID_Ajustar_XGB.iypnb\".\n",
    "        params_XGB = {\n",
    "                'subsample': 0.8,\n",
    "                'reg_lambda': 50,\n",
    "                'min_child_weight': 3,\n",
    "                'max_depth': 5,\n",
    "                'learning_rate': 0.5,\n",
    "                'gamma': 5,\n",
    "                'colsample_bytree': 0.8}\n",
    "        if (tuned == True):\n",
    "            modelo = XGBClassifier(**params_XGB)\n",
    "            print(\"Selecionado modelo XGBClassifier Ajustado.\")\n",
    "        else:\n",
    "            modelo = XGBClassifier()\n",
    "            print(\"Selecionado modelo XGBClassifier padrão.\")\n",
    "    elif classificador==\"2\":\n",
    "        # LogisticRegression\n",
    "        # Parametros obtidos através do notebook \"COVID_Ajustar_Logistic_Regression.ipynb\"\n",
    "        params_lr = {'C': 1.0, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
    "        if (tuned == True):\n",
    "            modelo = LogisticRegression(**params_lr)\n",
    "            print(\"Selecionado modelo LogisticRegression Ajustado.\")\n",
    "        else:\n",
    "            modelo = LogisticRegression()\n",
    "            print(\"Selecionado modelo LogisticRegression padrão.\")\n",
    "    elif classificador == \"3\":  \n",
    "        # KNN\n",
    "        modelo = KNeighborsClassifier(n_neighbors=4)\n",
    "        print(\"Selecionado modelo KNN.\") \n",
    "    elif classificador == \"4\":\n",
    "        # RANDOM FOREST\n",
    "        modelo = RandomForestClassifier(n_estimators=100, max_depth=6, random_state=42).fit(X_train, np.ravel(y_train))\n",
    "        print(\"Selecionado modelo RANDOM FOREST.\")\n",
    "    elif classificador == \"5\":    \n",
    "        # DecisionTree\n",
    "        # Parametros obtidos através do notebook \"COVID_Ajustar_DecisionTree.ipynb\"\n",
    "        params_DecisionTree = {'ccp_alpha':0.0,\n",
    "                'class_weight':None,\n",
    "                'criterion':'gini',\n",
    "                'max_depth':15,\n",
    "                'max_features':None,\n",
    "                'max_leaf_nodes':None,\n",
    "                'min_impurity_decrease':0.0,\n",
    "                'min_impurity_split':None,\n",
    "                'min_samples_leaf':1, #padrao\n",
    "                'min_samples_split':2, #padrao\n",
    "                'min_weight_fraction_leaf':0.0,\n",
    "                'presort':'deprecated',\n",
    "                'random_state':42,\n",
    "                'splitter':'best'\n",
    "        }\n",
    "        if (tuned == True):      \n",
    "            modelo =  DecisionTreeClassifier(**params_DecisionTree)\n",
    "            print(\"Selecionado modelo DecisionTreeClassifier ajustado.\")             \n",
    "        else:\n",
    "            modelo =  DecisionTreeClassifier()            \n",
    "            print(\"Selecionado modelo DecisionTreeClassifier padrão.\")       \n",
    "    elif classificador == \"6\":  \n",
    "        #Importa o modelo svm \n",
    "        from sklearn import svm\n",
    "        #Cria o classficador svm\n",
    "        modelo = svm.SVC(kernel='linear') # Linear Kernel\n",
    "    elif classificador == \"7\": \n",
    "        #ExtraTreesClassifier\n",
    "        if (tuned == True):    \n",
    "            #Parametros obtidos através do notebook \"COVID_Ajustar_ExtraTreesClassifier.ipynb\"     \n",
    "            params_XTC = {'criterion': 'entropy',\n",
    "                'max_depth': 55,\n",
    "                'max_features': 40,\n",
    "                'min_samples_split': 16,\n",
    "                'n_estimators': 300}\n",
    "\n",
    "            modelo = ExtraTreesClassifier(**params_XTC)\n",
    "            print(\"Selecionado modelo ExtraTreesClassifier ajustado.\")\n",
    "        else:\n",
    "            modelo = ExtraTreesClassifier()\n",
    "            print(\"Selecionado modelo ExtraTreesClassifier padrão.\")  \n",
    "    else:\n",
    "        print (\"Modelo inválido.\")\n",
    "        return\n",
    "    #Treina o modelo usando os dados de treinamento recebidos como parâmetros.\n",
    "    modelo.fit(X_train, y_train)  # treino com dados sem balanceamento\n",
    "\n",
    "    return modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "###########################################################\n",
    "# Esta função avalia a performance de um modelo\n",
    "############################################################\n",
    "def avalia_modelo(X_test,y_test,modelo,rank_score, id):\n",
    "    \"\"\"Esta função avalia a performance de um modelo. Parâmetros: X_test,y_test,modelo,rank_score, id \"\"\"\n",
    "    y_pred = modelo.predict(X_test)\n",
    "    plot_confusion_matrix(modelo, X_test, y_test,cmap=plt.cm.Blues, values_format='d')\n",
    "    plt.ylabel('Verdadeiro')\n",
    "    plt.xlabel('Previsto')\n",
    "    plt.xticks([0, 1], ['Não','Sim'])\n",
    "    plt.yticks([0, 1], ['Não','Sim'])\n",
    "\n",
    "    # Exibição de métricas de classificação:\n",
    "    model_name = type(modelo).__name__\n",
    "    indice = model_name + id \n",
    "    y_pred = modelo.predict(X_test)\n",
    "    rank_score[indice] = classification_report(y_test, y_pred)\n",
    "    print(rank_score[indice])\n",
    "    return rank_score"
   ]
  },
  {
   "source": [
    "### FEATURE SELECTION"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "####################################################################################\n",
    "# Esta função identifica, através de \"Eliminação Recursiva de Variáveis\" (RFE), o número ótimo de variáveis\n",
    "####################################################################################\n",
    "from sklearn.feature_selection import RFE\n",
    "def obtem_num_features(covid_df_pre_dummies, modelo):\n",
    "    \"\"\"Esta função identifica, através de \"Eliminação Recursiva de Variáveis\" (RFE), o número ótimo de variáveis.\n",
    "      Parâmetros: covid_df_pre_dummies, modelo\"\"\"   \n",
    "    labels = covid_df_pre_dummies.columns[:-1]  #última coluna comtém o atributo dependente (OBT_COVID)\n",
    "    X = covid_df_pre_dummies[labels] #dataframe com todas as colunas exceto a variável alvo\n",
    "    y = covid_df_pre_dummies['OBT_COVID'].astype('category') #ndarray com a variável a ser classificada.\n",
    "      #no de features\n",
    "    num_max = covid_df_pre_dummies.shape[1]\n",
    "    nof_list=np.arange(1,num_max)            \n",
    "    high_score=0\n",
    "    #Variavel para armazenar o numero otimo de features\n",
    "    nof=0           \n",
    "    score_list =[]\n",
    "    for n in range(len(nof_list)):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.25, random_state = 0)\n",
    "        model = modelo\n",
    "        rfe = RFE(model,nof_list[n])\n",
    "        X_train_rfe = rfe.fit_transform(X_train,y_train)\n",
    "        X_test_rfe = rfe.transform(X_test)\n",
    "        model.fit(X_train_rfe,y_train)\n",
    "        score = model.score(X_test_rfe,y_test)\n",
    "        score_list.append(score)\n",
    "        if(score>high_score):\n",
    "            high_score = score\n",
    "            nof = nof_list[n]\n",
    "    print(\"Num. ótimo de variáveis: %d\" %nof)\n",
    "    print(\"Score com %d variáveis: %f\" % (nof, high_score))\n",
    "    return nof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################\n",
    "# Esta função utiliza \"Eliminação Recursiva de Variáveis\" (RFE) para identificar as \n",
    "# melhores variáveis\n",
    "####################################################################################\n",
    "def sel_features(covid_df_pre_dummies,modelo, num_feat):\n",
    "    \"\"\"Esta função utiliza \"Eliminação Recursiva de Variáveis\" (RFE) para identificar as \n",
    "       melhores variáveis.\n",
    "       Parâmetros: covid_df_pre_dummies,modelo, num_feat\n",
    "       \"\"\"\n",
    "    #Metodo RFE para eliminação recursiva de variáveis\n",
    "    from sklearn.feature_selection import RFE\n",
    "    labels = covid_df_pre_dummies.columns[:-1]  #última coluna comtém o atributo dependente (OBT_COVID)\n",
    "    X = covid_df_pre_dummies[labels] #dataframe com todas as colunas exceto a variável alvo\n",
    "    y = covid_df_pre_dummies['OBT_COVID'].astype('category') #ndarray com a variável a ser classificada.\n",
    "    #Inicializa o modelo RFE\n",
    "    rfe = RFE(modelo, num_feat)\n",
    "    #Transforma os dados usando RFE\n",
    "    X_rfe = rfe.fit_transform(X,y)  \n",
    "    #Treina o modelo\n",
    "    modelo.fit(X_rfe,y)\n",
    "    print(rfe.support_)\n",
    "    print(rfe.ranking_)\n",
    "    return rfe.ranking_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################\n",
    "#Esta função exibe as ocorrências de valores únicos em cada coluna da dataframe\n",
    "####################################################################################\n",
    "def sel_features_importance(covid_df,model,k_best=10):\n",
    "    \"\"\"Esta função seleciona as \"melhores\" variáveis para o modelo passado como parametro.\n",
    "    Parâmetros: covid_df,model,k_best=10 \"\"\"\n",
    "    X = covid_df.iloc[:,:-1]  #variáveis independentes\n",
    "    y = covid_df.iloc[:,-1].astype(int) #variável alvo, última coluna. Converte para int devido a SelectKBest\n",
    "    model.fit(X,y)\n",
    "    #Monta o gráfico para melhor visualização da importância das variáveis \n",
    "    feat_importance = pd.Series(model.feature_importances_, index=X.columns)\n",
    "    feat_importance.nlargest(k_best).plot(kind='barh')  #prepara o grafico\n",
    "    plt.show() #exibe o grafico\n",
    "    list_best = feat_importance.nlargest(k_best).index.tolist() #retorna os nomes das variáveis em forma de lista\n",
    "    return list_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "####################################################################################\n",
    "# Esta função seleciona, genericamente, as variáveis com maior correlação com a variável dependente. \n",
    "####################################################################################\n",
    "def sel_univariate_features(covid_df,k_best=10):\n",
    "    \"\"\"Esta função seleciona, genericamente, as variáveis com maior correlação com a variável dependente.\n",
    "       Parâmetros: covid_df,k_best\"\"\"\n",
    "    #garante a normalizacao dos dados, pressuposto de SelectKBest\n",
    "    covid_df= normaliza_dados(covid_df)\n",
    "    #presume-se que a última variável (-1) do dataframe é a alvo\n",
    "    labels = covid_df.columns[:-1]  \n",
    "    col_cat = []  #armazenará os índices\n",
    "    for val in labels:\n",
    "        idx_col = covid_df.columns.get_loc(val) #obtém os respectivos índices\n",
    "        col_cat.append(idx_col) \n",
    "\n",
    "    X = covid_df.iloc[:,:-1]  #variáveis independentes\n",
    "    y = covid_df.iloc[:,-1].astype(int) #variável alvo, última coluna. Converte para int devido a SelectKBest\n",
    "\n",
    "    #Aplica a classe SelectKBest para extrair as \"k_best\" melhores variáveis\n",
    "    bestfeatures = SelectKBest(score_func=chi2, k=k_best)\n",
    "    fit = bestfeatures.fit(X,y)\n",
    "    dfscores = pd.DataFrame(fit.scores_)\n",
    "    dfcolumns = pd.DataFrame(X.columns)\n",
    "    #concat os dois dataframes para melhor visualização \n",
    "    featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "    featureScores.columns = ['variavel','Score']  #nomeaia as colunas do dataframe \n",
    "    print(featureScores.nlargest(k_best,'Score'))  #imprime as k_best melhores\n",
    "    lista_best = featureScores.nlargest(k_best,'Score')['variavel'].tolist() # prepara um ndarray das melhores variáveis para retornar\n",
    "    return lista_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################\n",
    "# Esta função seleciona as melhores variáveis para o modelo passado como parametro. \n",
    "####################################################################################\n",
    "def sel_lim_features_importance(covid_df,model,lim=0.01):\n",
    "    \"\"\"Esta função seleciona as melhores variáveis para o modelo passado como parametro \"\"\"\n",
    "    X = covid_df.iloc[:,:-1]  #variáveis independentes\n",
    "    y = covid_df.iloc[:,-1].astype(int) #variável alvo, última coluna. Converte para int devido a SelectKBest\n",
    "    model.fit(X,y)\n",
    "    #Monta o gráfico para melhor visualização da importância das variáveis \n",
    "    feat_importance = pd.Series(model.feature_importances_, index=X.columns)\n",
    "    indice = []\n",
    "    for i in range(len(feat_importance)):\n",
    "        if feat_importance[i] >= lim:\n",
    "            indice.append(i)\n",
    "    return indice"
   ]
  }
 ]
}